{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <h1>Base SIRENE</h1>\n",
    "</div>\n",
    "\n",
    "\n",
    "La **base Sirene** est le fournisseur des données d’identité des entreprises et des établissements. \n",
    "Elle fait partie des données de référence du Service public de la donnée mis en place par la loi pour une République numérique.\n",
    "Voir plus d'informations sur le site de **[l'Insee](https://www.insee.fr/fr/information/3591226#:~:text=Elle%20donne%20acc%C3%A8s%20aux%20donn%C3%A9es,r%C3%A9pertoire%20interadministratif%20Sirene%20depuis%201973.)**.\n",
    "\n",
    "Le site [sirene.fr](https://www.sirene.fr/sirene/public/accueil) permet de créer en ligne des fichiers comprenant au maximum 200 000 établissements, ce qui n'est pas assez pour nos besoins ici, il faut donc tout télécharger sur [data gouv](https://www.data.gouv.fr/fr/datasets/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/).\n",
    "\n",
    "Le but de ce notebook est de télécharger cette base de données et de la nettoyer pour obtenir les informations suivantes :\n",
    "\n",
    "- **SIRET** : code d'identification d'un établissement. On distingue ici les entreprises (identifiées par leur numéro SIREN) des établissements (SIRET = SIREN + NIC)\n",
    "- **code NAF/APE** : activité principale de l'établissement renseignée à l'INPI\n",
    "- **code commune INSEE** : code de la commune pour pouvoir localiser les entreprises\n",
    "- **dénomination  / nom commercial** : nom de l'entreprise \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q matplotlib seaborn duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Téléchargement des données\n",
    "\n",
    "On télécharge les données via ce [lien permanent](https://www.data.gouv.fr/fr/datasets/r/0651fb76-bcf3-4f6a-a38d-bc04fa708576) aux serveurs de DataGouv.\n",
    "\n",
    "La documentation relative au jeu de données est disponible [ici](https://static.data.gouv.fr/resources/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/20230523-095315/description-fichier-stocketablissement.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.system(\"wget -q -P '../Données nationales/' https://www.data.gouv.fr/fr/datasets/r/0651fb76-bcf3-4f6a-a38d-bc04fa708576\")\n",
    "os.system(\"wget -q -P '../Données nationales/' https://static.data.gouv.fr/resources/base-sirene-des-entreprises-et-de-leurs-etablissements-siren-siret/20230523-095315/description-fichier-stocketablissement.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification que le téléchargement s'est correctement effectué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier a été téléchargé correctement\n"
     ]
    }
   ],
   "source": [
    "# id du document\n",
    "id_doc = '0651fb76-bcf3-4f6a-a38d-bc04fa708576'\n",
    "\n",
    "# hash du document\n",
    "hash_datagouv = \"c709c26eefa07043f5fb5cf74816ec01dc4739f37589a1c02a70ab6583cb37ae\"\n",
    "\n",
    "# on hash le document (sha256)\n",
    "hash_local = hashlib.sha256(open('../Données nationales/'+id_doc, 'rb').read()).hexdigest()\n",
    "\n",
    "# on vérifie que le hash du document téléchargé correspond au hash connu\n",
    "if hash_datagouv == hash_local:\n",
    "    print(\"Le fichier a été téléchargé correctement\")\n",
    "else:\n",
    "    print(\"Le fichier téléchargé est corrompu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Décompression du fichier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"unzip -q -o '../Données nationales/\"+id_doc+\"' -d '../Données nationales/'\")\n",
    "os.remove('../Données nationales/'+id_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion en parquet pour optimiser la mémoire et le traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a bien 48 colonnes dans le fichier\n",
      "Le fichier pèse 6.37 Go\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../Données nationales/StockEtablissement_utf8.csv',nrows=100)\n",
    "\n",
    "# la documentation indique qu'il y a 48 colonnes\n",
    "if df.shape[1] == 48:\n",
    "    print(\"Il y a bien 48 colonnes dans le fichier\")\n",
    "else:\n",
    "    print(\"Vérifier la lecture du fichier\")\n",
    "\n",
    "# poids total du fichier en Go\n",
    "poids = os.path.getsize('../Données nationales/StockEtablissement_utf8.csv')/1024**3\n",
    "print(\"Le fichier pèse {:.2f} Go\".format(poids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des variables que l'on souhaite garder et des transformations à appliquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nom des variables d'intérêt\n",
    "desired_col_list =[\"siret\",\"numeroVoieEtablissement\",\"typeVoieEtablissement\",\"libelleVoieEtablissement\",\n",
    "                   \"codePostalEtablissement\",\"libelleCommuneEtablissement\",\"codeCommuneEtablissement\",\n",
    "                    \"activitePrincipaleEtablissement\",\"denominationUsuelleEtablissement\",\"etatAdministratifEtablissement\",\n",
    "                    \"enseigne1Etablissement\",\"enseigne2Etablissement\",\"enseigne3Etablissement\",\n",
    "                    \"statutDiffusionEtablissement\",\"nomenclatureActivitePrincipaleEtablissement\"]\n",
    "\n",
    "\n",
    "# position des colonnes à enlever\n",
    "col_to_drop = [index for index, nom in enumerate(df.columns) if nom not in desired_col_list]\n",
    "\n",
    "# position des colonnes à garder\n",
    "col_to_keep = [index for index, nom in enumerate(df.columns) if nom in desired_col_list]\n",
    "\n",
    "# transformations à appliquer sur les colonnes\n",
    "# en key on met le nom de la colonne dans le fichier final, en value on met le nom de la / des colonnes dans le fichier initial\n",
    "# si la value est une liste, on concatène les colonnes avec un espace\n",
    "col_transform = {\"siret\":\"siret\",\n",
    "                 \"adresseEtablissement\": [\"numeroVoieEtablissement\",\"typeVoieEtablissement\",\"libelleVoieEtablissement\",\n",
    "                                            \"codePostalEtablissement\",\"libelleCommuneEtablissement\"],\n",
    "                \"codeCommuneEtablissement\":\"codeCommuneEtablissement\",\n",
    "                \"activitePrincipaleEtablissement\":\"activitePrincipaleEtablissement\",\n",
    "                \"enseigneEtablissement\":[\"enseigne1Etablissement\",\"enseigne2Etablissement\",\"enseigne3Etablissement\"],  \n",
    "                \"denominationUsuelleEtablissement\":\"denominationUsuelleEtablissement\",\n",
    "                \"etatAdministratifEtablissement\":\"etatAdministratifEtablissement\",\n",
    "                \"statutDiffusionEtablissement\":\"statutDiffusionEtablissement\",\n",
    "                \"nomenclatureActivitePrincipaleEtablissement\":\"nomenclatureActivitePrincipaleEtablissement\"}\n",
    "\n",
    "\n",
    "\n",
    "# fonction qui permet de transformer les colonnes\n",
    "def transform_col(df,col_to_drop,col_transform):\n",
    "\n",
    "    # on enlève les colonnes qui ne nous intéressent pas\n",
    "    df = df.drop(df.columns[col_to_drop],axis=1)\n",
    "\n",
    "    # on transforme les colonnes\n",
    "    for key, value in col_transform.items():\n",
    "\n",
    "        if isinstance(value, list):\n",
    "\n",
    "            # join columns with a space if value is a list, replace NaN with empty string\n",
    "            df[key] = df[value].apply(lambda x: ' '.join(x.dropna().astype(str)),axis=1)\n",
    "\n",
    "        else:\n",
    "            df[key] = df[value]\n",
    "\n",
    "    # on ne garde que les colonne finales\n",
    "    df = df[list(col_transform.keys())]\n",
    "\n",
    "    # on elimine les doublons\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # on elimine les etablissements fermés\n",
    "    df = df[df[\"etatAdministratifEtablissement\"] == \"A\"]\n",
    "\n",
    "    # on enlève la colonne etatAdministratifEtablissement\n",
    "    df = df.drop([\"etatAdministratifEtablissement\"],axis=1)\n",
    "\n",
    "    # definition des types de variables\n",
    "    df = df.astype({\"siret\":int,\n",
    "                    \"adresseEtablissement\":str,\n",
    "                    \"codeCommuneEtablissement\":str,\n",
    "                    \"activitePrincipaleEtablissement\":str,\n",
    "                    \"enseigneEtablissement\":str,\n",
    "                    \"denominationUsuelleEtablissement\":str,\n",
    "                    \"statutDiffusionEtablissement\":str,\n",
    "                    \"nomenclatureActivitePrincipaleEtablissement\":str})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sur un échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les colonnes finales sont bien dans le dataframe\n",
      "CPU times: user 7.36 s, sys: 95.7 ms, total: 7.46 s\n",
      "Wall time: 7.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# test de la fonction sur un échantillon\n",
    "df = pd.read_csv('../Données nationales/StockEtablissement_utf8.csv',nrows=100000,low_memory=False)\n",
    "\n",
    "df = transform_col(df,col_to_drop,col_transform)\n",
    "\n",
    "# colonnes finales = colonnes du dictionnaire - colonne etatAdministratifEtablissement\n",
    "final_col = list(col_transform.keys())\n",
    "final_col.remove(\"etatAdministratifEtablissement\")\n",
    "                                           \n",
    "\n",
    "# on vérifie que les colonnes finales sont bien dans le dataframe\n",
    "if set(final_col).issubset(set(df.columns)):\n",
    "    print(\"Les colonnes finales sont bien dans le dataframe\")\n",
    "else:\n",
    "    print(\"Les colonnes finales ne sont pas dans le dataframe\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix du nombre de chunks : d'après le test précédent, il est cohérent de choisir des chunks de 100 000 lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 37768658\n",
      "Number of chunks: 377\n"
     ]
    }
   ],
   "source": [
    "# Command to get the number of lines in the CSV file\n",
    "command = \"wc -l '../Données nationales/StockEtablissement_utf8.csv' | awk '{print $1}'\"\n",
    "\n",
    "# Run the command and capture the output\n",
    "output = subprocess.check_output(command, shell=True)\n",
    "\n",
    "# Convert the output to an integer\n",
    "number_of_lines = int(output.decode('utf-8').strip())\n",
    "\n",
    "# Calculate the number of chunks (assuming chunk size of 100000)\n",
    "chunk_size = 100000\n",
    "number_of_chunks = number_of_lines // chunk_size\n",
    "\n",
    "# Print or use the variable as needed\n",
    "print(f\"Number of lines: {number_of_lines}\")\n",
    "print(f\"Number of chunks: {number_of_chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation du csv et sauvegarde en parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "378it [1:18:36, 12.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# on lit le csv en chunks\n",
    "csv_reader = pd.read_csv('../Données nationales/StockEtablissement_utf8.csv',chunksize=chunk_size,low_memory=False)\n",
    "\n",
    "# on initialise le dataframe final\n",
    "df_final = pd.DataFrame(columns=final_col)\n",
    "\n",
    "# list des chunks\n",
    "chunks_list = [transform_col(chunk, col_to_drop, col_transform) for chunk in tqdm(csv_reader)]\n",
    "\n",
    "# on enregistre le dataframe final\n",
    "df_final = pd.concat(chunks_list,ignore_index=True)\n",
    "\n",
    "# on enregistre le dataframe final\n",
    "df_final.to_parquet('../Données nationales/StockEtablissement_utf8.parquet',compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification du bien fondé de l'opération précédente et suppression du csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier parquet pèse 0.34 Go\n",
      "Le fichier csv pèse 6.37 Go\n"
     ]
    }
   ],
   "source": [
    "# poids des fichiers en Go\n",
    "poids = os.path.getsize('../Données nationales/StockEtablissement_utf8.parquet')/1024**3\n",
    "print(\"Le fichier parquet pèse {:.2f} Go\".format(poids))\n",
    "poids = os.path.getsize('../Données nationales/StockEtablissement_utf8.csv')/1024**3\n",
    "print(\"Le fichier csv pèse {:.2f} Go\".format(poids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_zone_de_chalandise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
